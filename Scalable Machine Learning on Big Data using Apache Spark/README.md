This course will empower us with the skills to scale data science and machine learning (ML) tasks on Big Data sets using Apache Spark. Most real world machine learning work involves very large data sets that go beyond the CPU, memory and storage limitations of a single computer. 

Apache Spark is an open source framework that leverages cluster computing and distributed storage to process extremely large data sets in an efficient and cost effective manner. Therefore an applied knowledge of working with Apache Spark is a great asset and potential differentiator for a Machine Learning engineer.

After completing this course, we will be able to:
- gain a practical understanding of Apache Spark, and apply it to solve machine learning problems involving both small and big data
- understand how parallel code is written, capable of running on thousands of CPUs. 
- make use of large scale compute clusters to apply machine learning algorithms on Petabytes of data using Apache SparkML Pipelines. 
- eliminate out-of-memory errors generated by traditional machine learning frameworks when data doesn’t fit in a computer's main memory
- test thousands of different ML models in parallel to find the best performing one – a technique used by many successful Kagglers
- (Optional) run SQL statements on very large data sets using Apache SparkSQL and the Apache Spark DataFrame API.

Prerequisites:
- basic python programming
- basic machine learning (optional introduction videos are provided in this course as well)
- basic SQL skills for optional content
